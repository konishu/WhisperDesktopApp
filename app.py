import streamlit as st
# import anthropic
import pandas as pd
from scipy.io import wavfile


# whiper
import gradio as gr
from faster_whisper import WhisperModel

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã™ã‚‹
from tempfile import NamedTemporaryFile

# éŸ³å£°å†ç”Ÿ
import base64
import time
import io

# whisperã®å‡¦ç†
model_size = "tiny"
model = WhisperModel(model_size, device="cpu", compute_type="int8")


with st.sidebar:
    st.markdown("""
      ## æ¦‚è¦
      éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¢ãƒ—ãƒªã§ã™ã€‚

      ### ä½¿ã„æ–¹
     1. mp3/mp4ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
     2. å‡¦ç†ã•ã‚ŒãŸçµæœãŒä¸‹ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
    """)


st.title("ğŸ™ï¸ğŸ™ï¸ğŸ™ï¸æ–‡å­—èµ·ã“ã—ã‚¢ãƒ—ãƒªğŸ™ï¸ğŸ™ï¸ğŸ™ï¸")
uploaded_file = st.file_uploader("Upload an article", type=["mp4","mp3"])
st.info("ğŸ“å‡¦ç†ãŒçµ‚ã‚ã‚Œã°ã€ä»¥ä¸‹ã«çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™")


result = pd.DataFrame(
        {'start': [], 'end': [], 'text': []},
        index=[])

def spacing():
    st.markdown("<br></br>", unsafe_allow_html=True)

if uploaded_file is not None:
  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®transcribeã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å…¥åŠ›ã«ã™ã‚‹ã€‚ãã®ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã™ã‚‹
    with NamedTemporaryFile(suffix="mp3") as temp:
        temp.write(uploaded_file.getvalue())
        temp.seek(0)
        # st.audio(uploaded_file.getvalue(),start_time=2)
        segments,info  = model.transcribe(temp.name,language='ja',beam_size=5)
        for segment in segments:
            result.loc[str(segment.id)] = [segment.start,segment.end,segment.text]


    for i in range(len(result)):
        st.markdown(
            f"""
            <h4 style='text-align: center; color: black;'>{result['start'][i]}-{result['end'][i]}</h4>
            """,
            unsafe_allow_html=True,
        )
        spacing()
        st.markdown(
            f"""
            <h5 style='text-align: center; color: black;'>{result['text'][i]}</h4>
            """,
            unsafe_allow_html=True,
        )
        st.audio(uploaded_file.getvalue(),start_time=int(result['start'][i]))
        st.markdown("---")
    
    


st.table(result)





# st.markdown(result.to_html(escape=False), unsafe_allow_html=True)


# audio_placeholder = st.empty()
# if uploaded_file is not None:
#     audio_str = "data:audio/ogg;base64,%s"%(base64.b64encode(uploaded_file.getvalue()).decode())
#     audio_html = """
#                     <audio controls>
#                     <source src="%s" type="audio/ogg">
#                     Your browser does not support the audio element.
#                     </audio>
#                 """ %audio_str
#     audio_placeholder.empty()
#     time.sleep(0.5) #ã“ã‚ŒãŒãªã„ã¨ä¸Šæ‰‹ãå†ç”Ÿã•ã‚Œã¾ã›ã‚“
#     audio_placeholder.markdown(audio_html, unsafe_allow_html=True)











# def transcribe(audio):

#     # load audio and pad/trim it to fit 30 seconds
#     audio = whisper.load_audio(audio)
#     audio = whisper.pad_or_trim(audio)

#     # make log-Mel spectrogram and move to the same device as the model
#     mel = whisper.log_mel_spectrogram(audio).to(model.device)

#     # detect the spoken language
#     _, probs = model.detect_language(mel)
#     print(f"Detected language: {max(probs, key=probs.get)}")

#     # decode the audio
#     options = whisper.DecodingOptions()
#     result = whisper.decode(model, mel, options)
#     return result.text