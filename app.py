import streamlit as st
# import anthropic
import pandas as pd

# whiper
import gradio as gr
from faster_whisper import WhisperModel

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã™ã‚‹
from tempfile import NamedTemporaryFile

# éŸ³å£°å†ç”Ÿ
import base64
import time

# whisperã®å‡¦ç†
model_size = "tiny"
model = WhisperModel(model_size, device="cpu", compute_type="int8")


with st.sidebar:
    st.markdown("""
      ## æ¦‚è¦
      éŸ³å£°æ–‡å­—èµ·ã“ã—ã‚¢ãƒ—ãƒªã§ã™ã€‚

      ### ä½¿ã„æ–¹
     1. mp3/mp4ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚
     2. å‡¦ç†ã•ã‚ŒãŸçµæœãŒä¸‹ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
    """)


st.title("ğŸ™ï¸ğŸ™ï¸ğŸ™ï¸æ–‡å­—èµ·ã“ã—ã‚¢ãƒ—ãƒªğŸ™ï¸ğŸ™ï¸ğŸ™ï¸")
uploaded_file = st.file_uploader("Upload an article", type=["mp4","mp3"])
st.info("ğŸ“å‡¦ç†ãŒçµ‚ã‚ã‚Œã°ã€ä»¥ä¸‹ã«çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™")



result = pd.DataFrame(
        {'start': [], 'end': [], 'text': []},
        index=[])


if uploaded_file is not None:
  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®transcribeã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å…¥åŠ›ã«ã™ã‚‹ã€‚ãã®ãŸã‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã™ã‚‹
  with NamedTemporaryFile(suffix="mp3") as temp:
        temp.write(uploaded_file.getvalue())
        temp.seek(0)
        st.audio(uploaded_file.getvalue())
        segments,info  = model.transcribe(temp.name,language='ja',beam_size=5)
        for segment in segments:
            result.loc[str(segment.id)] = [segment.start,segment.end,segment.text]
            # result.append([segment.id,segment.start,segment.end,segment.text])
        # st.write(result["text"])

st.table(result)
# st.markdown(result.to_html(escape=False), unsafe_allow_html=True)


audio_placeholder = st.empty()
if uploaded_file is not None:
    audio_str = "data:audio/ogg;base64,%s"%(base64.b64encode(uploaded_file.getvalue()).decode())
    audio_html = """
                    <audio controls autoplay=False>
                    <source src="%s" type="audio/ogg" autoplay=False>
                    Your browser does not support the audio element.
                    </audio>
                """ %audio_str
    audio_placeholder.empty()
    time.sleep(0.5) #ã“ã‚ŒãŒãªã„ã¨ä¸Šæ‰‹ãå†ç”Ÿã•ã‚Œã¾ã›ã‚“
    audio_placeholder.markdown(audio_html, unsafe_allow_html=True)











# def transcribe(audio):

#     # load audio and pad/trim it to fit 30 seconds
#     audio = whisper.load_audio(audio)
#     audio = whisper.pad_or_trim(audio)

#     # make log-Mel spectrogram and move to the same device as the model
#     mel = whisper.log_mel_spectrogram(audio).to(model.device)

#     # detect the spoken language
#     _, probs = model.detect_language(mel)
#     print(f"Detected language: {max(probs, key=probs.get)}")

#     # decode the audio
#     options = whisper.DecodingOptions()
#     result = whisper.decode(model, mel, options)
#     return result.text